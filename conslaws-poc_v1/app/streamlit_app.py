# -*- coding: utf-8 -*-
"""
streamlit_app.py
- ê²€ìƒ‰/ìƒì„± ë°ëª¨ + í‰ê°€(Eval) íƒ­
- ì§€í‘œ: nDCG@k / MRR@k / Recall@k / P95 latency
- ê°œì„ ì  ë°˜ì˜:
  1) nDCGê°€ ë‹¤ì¤‘ ì •ë‹µ(gold_ids) ì§€ì›
  2) ì—…ë¡œë“œ íŒŒì„œê°€ query/question, gold_ids/gold_id ëª¨ë‘ í—ˆìš©
  3) API í˜¸ì¶œ ì˜ˆì™¸ ì²˜ë¦¬ ê°•í™” + dict ì ‘ê·¼ ì•ˆì „í™”
  4) ëŒ€ìš©ëŸ‰ í‰ê°€ UX: ì§„í–‰ë¥ /CSV ë‚´ë³´ë‚´ê¸°
  5) cand_factor(ë¦¬ë­í¬ í›„ë³´í­) ì•ˆë‚´
"""
from __future__ import annotations


import os
import json
import math
import time
from typing import List, Dict, Any, Optional

import numpy as np
import pandas as pd
import streamlit as st
import httpx

# ---------------------------- ì„¤ì • ----------------------------
API_DEFAULT = os.environ.get("API_URL", "http://localhost:8000")
API = st.secrets.get("API_URL", API_DEFAULT )

st.set_page_config(page_title="ê±´ì„¤ ë²•ë ¹ RAG", layout="wide")
st.title("ğŸ—ï¸ ê±´ì„¤ ë²•ë ¹ RAG")
st.sidebar.markdown("### âš™ï¸ ì„¤ì •")
st.sidebar.write(f"**API**: `{API}`")

# ê³µí†µ ì˜µì…˜(ì‚¬ì´ë“œë°”)
k = st.sidebar.slider("Top-k", min_value=3, max_value=30, value=10, step=1)
rerank = st.sidebar.checkbox("ë¦¬ë­í¬ ì‚¬ìš©(CrossEncoder)", value=True)
cand_factor = st.sidebar.slider("cand_factor (ë¦¬ë­í¬ í›„ë³´í­ = kÃ—cand_factor)", min_value=1.0, max_value=5.0, value=2.0, step=0.1)
warmup = st.sidebar.number_input("Warmup(í‰ê°€ ì „ ì˜ˆì—´ í˜¸ì¶œ ìˆ˜)", min_value=0, max_value=20, value=2, step=1)
st.sidebar.caption(f"ì‹¤ì œ ë¦¬ë­í¬ í›„ë³´ ìˆ˜ â‰ˆ **{int(round(k * max(1.0, cand_factor)))}**")

# (ì„ íƒ) ìƒì„± ë°±ì—”ë“œ/ëª¨ë¸
with st.sidebar.expander("ìƒì„± ëª¨ë¸ (ì„ íƒ)", expanded=False):
    gen_backend = st.text_input("gen_backend", value="auto")
    gen_model = st.text_input("gen_model", value="gpt-4o-mini")

# ---------------------------- ìœ í‹¸ í•¨ìˆ˜ ----------------------------
def _safe_get_candidates(sr: Any) -> List[Dict[str, Any]]:
    """
    ë‹¤ì–‘í•œ í˜•íƒœì˜ /search ì‘ë‹µì—ì„œ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ë¥¼ ìµœëŒ€í•œ ìœ ì—°í•˜ê²Œ ì¶”ì¶œ
    ì˜ˆìƒ í˜•íƒœ:
      - {"results": [ {...}, ... ]}
      - {"hits": [ {...}, ... ]}
      - [ {...}, ... ]
      - {"items": [ ... ]}
    """
    if isinstance(sr, list):
        return [x for x in sr if isinstance(x, dict)]
    if isinstance(sr, dict):
        for key in ("results", "hits", "items"):
            if isinstance(sr.get(key), list):
                return [x for x in sr[key] if isinstance(x, dict)]
        # ë‹¨ì¼ ê°ì²´ì¼ ìˆ˜ë„ ìˆìŒ
        if "id" in sr or "_id" in sr:
            return [sr]
    return []


def _extract_id(rec: Dict[str, Any]) -> Optional[str]:
    return rec.get("id") or rec.get("_id") or rec.get("doc_id")

# ===================== Eval helpers (ê°„ë‹¨ ë©”íŠ¸ë¦­ ê³„ì‚°) ======================
def _dcg_at_k(rels, k=10):
    return sum((1.0 if r else 0.0) / math.log2(i + 2) for i, r in enumerate(rels[:k]))

def _ndcg_at_k(pred_ids, gold_ids, k=10):
    if not gold_ids:
        return 0.0
    rels = [1 if pid in gold_ids else 0 for pid in pred_ids[:k]]
    dcg = _dcg_at_k(rels, k)
    m = min(len(gold_ids), k)
    ideal_rels = [1] * m + [0] * max(0, k - m)
    idcg = _dcg_at_k(ideal_rels, k)
    return (dcg / idcg) if idcg else 0.0

def _mrr_at_k(pred_ids: List[str], gold_ids: List[str], k: int = 10) -> float:
    """
    ì²« ì •ë‹µì˜ ì—­ìˆœìœ„ í‰ê· 
    """
    gold = set(gold_ids)
    for i, pid in enumerate(pred_ids[:k], 1):
        if pid in gold:
            return 1.0 / i
    return 0.0

def _recall_at_k(pred_ids: List[str], gold_ids: List[str], k: int = 10) -> float:
    if not gold_ids:
        return 0.0
    hit = len(set(pred_ids[:k]).intersection(set(gold_ids)))
    return hit / float(len(set(gold_ids)))


def _p95(values: List[float]) -> float:
    if not values:
        return 0.0
    return float(np.quantile(np.array(values, dtype=float), 0.95))


def _load_eval_jsonl(uploaded_file) -> List[Dict[str, Any]]:
    """
    ì—…ë¡œë” íŒŒì„œ: query/question, gold_ids/gold_id ëª¨ë‘ í—ˆìš©
    - IRìš© í¬ë§·: {"query": "...", "gold_ids": ["uuid", ...]}
    - í˜¸í™˜: {"question": "..."} (gold_ids ë¹„ì–´ ìˆìœ¼ë©´ ì§€í‘œëŠ” 0)
    """
    raw = uploaded_file.read()
    text = raw.decode("utf-8", errors="ignore") if isinstance(raw, (bytes, bytearray)) else str(raw)
    items = []
    for line in text.splitlines():
        line = line.strip()
        if not line:
            continue
        row = json.loads(line)
        q = row.get("query") or row.get("question")
        gold_ids = row.get("gold_ids")
        if gold_ids is None:
            gid = row.get("gold_id")
            gold_ids = [gid] if gid else []
        if q:
            items.append({"query": q, "gold_ids": [g for g in gold_ids if g]})
    return items
# ========================================================================
def _call_search(query: str, topk: int, rerank: bool, cand_factor: float) -> List[Dict[str, Any]]:
    try:
        resp = httpx.get(
            f"{API}/search",
            params={"q": query, "k": topk, "rerank": str(rerank).lower(), "cand_factor": cand_factor},
            timeout=120,
        )
        resp.raise_for_status()
        sr = resp.json()
    except Exception as e:
        st.error(f"ê²€ìƒ‰ í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return []
    return _safe_get_candidates(sr)


def _call_answer(query: str, topk: int, rerank: bool, cand_factor: float, gen_backend: str, gen_model: str) -> Dict[str, Any]:
    try:
        resp = httpx.post(
            f"{API}/answer",
            json={
                "query": query,
                "k": topk,
                "rerank": rerank,
                "include_context": True,
                "gen_backend": gen_backend,
                "gen_model": gen_model,
                "cand_factor": cand_factor,
            },
            timeout=180,
        )
        resp.raise_for_status()
        return resp.json()
    except Exception as e:
        st.error(f"ìƒì„± í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return {}


# =========================== Tabs: ê²€ìƒ‰ / í‰ê°€ ===========================
tab_search, tab_eval = st.tabs(["ğŸ” ê²€ìƒ‰", "ğŸ“Š í‰ê°€"])

# ============================ ğŸ” ê²€ìƒ‰ / ìƒì„± ============================
with tab_search:
    st.subheader("ê²€ìƒ‰")
    q = st.text_input("ì§ˆë¬¸/ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”", value="í•˜ë„ê¸‰ëŒ€ê¸ˆ ì§ì ‘ì§€ê¸‰ ìš”ê±´ì€?")
    col1, col2 = st.columns([1, 1])

    with col1:
        if st.button("ê²€ìƒ‰ ì‹¤í–‰", use_container_width=True):
            results = _call_search(q, k, rerank, cand_factor)
            if not results:
                st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                rows = []
                for i, r in enumerate(results[:k], 1):
                    rows.append({
                        "rank": i,
                        "id": _extract_id(r),
                        "score": r.get("score"),
                        "law_name": r.get("law_name"),
                        "clause_id": r.get("clause_id"),
                        "title": r.get("title"),
                        "text": (r.get("text") or "")[:220] + ("â€¦" if r.get("text") and len(r.get("text")) > 220 else "")
                    })
                st.dataframe(pd.DataFrame(rows), use_container_width=True, hide_index=True)

    with col2:
        if st.button("ìƒì„± ì‹¤í–‰(ë‹µë³€)", type="primary", use_container_width=True):
            ar = _call_answer(q, k, rerank, cand_factor, gen_backend, gen_model)
            if not ar:
                st.warning("ë‹µë³€ì´ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.markdown("#### ë‹µë³€")
                st.write(ar.get("answer", ""))

                citations = ar.get("citations", [])
                if citations:
                    st.markdown("##### ì¸ìš©(ë²•ë ¹/ì¡°ë¬¸)")
                    st.caption(", ".join([f"[{c.get('law','') or c.get('law_name','')} {c.get('clause_id','')}]" for c in citations]))

                used = ar.get("used_contexts") or ar.get("contexts") or []
                if used:
                    st.markdown("##### ì‚¬ìš© ì»¨í…ìŠ¤íŠ¸(ìƒìœ„ 3ê°œ)")
                    for i, c in enumerate(used[:3], 1):
                        st.write(f"**[{i}]** {(c.get('title') or c.get('clause_id') or '')}")
                        st.write((c.get("text") or "")[:500])

# ============================ ğŸ“Š í‰ê°€(Eval) ============================
with tab_eval:
    st.subheader("í‰ê°€ Â· IR ì§€í‘œ (nDCG/MRR/Recall/P95)")
    st.caption("ì—…ë¡œë“œ í¬ë§·: `{'query': '...', 'gold_ids': ['uuid1','uuid2',...]}` (ë˜ëŠ” `question`/`gold_id`ë„ í—ˆìš©)")

    up = st.file_uploader("eval_ids.jsonl ì—…ë¡œë“œ", type=["jsonl"])
    run = st.button("í‰ê°€ ì‹¤í–‰", type="primary")

    if up and run:
        items = _load_eval_jsonl(up)
        if not items:
            st.error("ìœ íš¨í•œ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
            st.stop()

        # gold ì—†ëŠ” í•­ëª© ì•ˆë‚´
        no_gold = [it for it in items if not it.get("gold_ids")]
        if no_gold:
            st.info(f"gold_idsê°€ ë¹„ì–´ìˆëŠ” í•­ëª© {len(no_gold)}ê°œê°€ ìˆìŠµë‹ˆë‹¤. í•´ë‹¹ í•­ëª©ì€ ì§€í‘œì— ë°˜ì˜ë˜ì§€ ì•Šê±°ë‚˜ 0ìœ¼ë¡œ ê³„ì‚°ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

        # Warmup
        if warmup > 0:
            st.write(f"Warmup {warmup}íšŒ ì§„í–‰ ì¤‘â€¦")
            for it in items[:warmup]:
                _ = _call_search(it["query"], k, rerank, cand_factor)
            st.success("Warmup ì™„ë£Œ")

        rows = []
        n_total = len(items)
        progress = st.progress(0.0)

        latencies_ms: List[float] = []
        ndcgs: List[float] = []
        mrrs: List[float] = []
        recalls: List[float] = []

        for idx, it in enumerate(items, 1):
            q = it["query"]
            gold_ids = it.get("gold_ids", [])
            t0 = time.perf_counter()
            results = _call_search(q, k, rerank, cand_factor)
            t1 = time.perf_counter()
            elapsed_ms = (t1 - t0) * 1000.0

            pred_ids = []
            show_rows = []
            for rnk, rec in enumerate(results[:k], 1):
                pid = _extract_id(rec)
                pred_ids.append(pid)
                show_rows.append({
                    "rank": rnk,
                    "id": pid,
                    "score": rec.get("score"),
                    "law_name": rec.get("law_name"),
                    "clause_id": rec.get("clause_id"),
                    "title": rec.get("title")
                })

            # ì§€í‘œ
            ndcg = _ndcg_at_k(pred_ids, gold_ids, k=k) if gold_ids else 0.0
            mrr_ = _mrr_at_k(pred_ids, gold_ids, k=k) if gold_ids else 0.0
            rec_ = _recall_at_k(pred_ids, gold_ids, k=k) if gold_ids else 0.0

            latencies_ms.append(elapsed_ms)
            if gold_ids:
                ndcgs.append(ndcg)
                mrrs.append(mrr_)
                recalls.append(rec_)

            rows.append({
                "query": q,
                "gold_ids": ", ".join(gold_ids) if gold_ids else "",
                "pred_ids(topk)": ", ".join([p for p in pred_ids if p]),
                "nDCG@k": round(ndcg, 4),
                "MRR@k": round(mrr_, 4),
                "Recall@k": round(rec_, 4),
                "latency_ms": round(elapsed_ms, 1)
            })

            progress.progress(idx / max(1, n_total))

        # ì§‘ê³„
        mean_ndcg = float(np.mean(ndcgs)) if ndcgs else 0.0
        mean_mrr = float(np.mean(mrrs)) if mrrs else 0.0
        mean_recall = float(np.mean(recalls)) if recalls else 0.0
        p95_lat = _p95(latencies_ms)
        avg_lat = float(np.mean(latencies_ms)) if latencies_ms else 0.0

        c1, c2, c3, c4, c5 = st.columns(5)
        c1.metric("nDCG@k", f"{mean_ndcg:.3f}")
        c2.metric("MRR@k", f"{mean_mrr:.3f}")
        c3.metric("Recall@k", f"{mean_recall:.3f}")
        c4.metric("P95 latency (ms)", f"{p95_lat:.1f}")
        c5.metric("Avg latency (ms)", f"{avg_lat:.1f}")

        st.divider()
        st.markdown("#### ì§ˆì˜ë³„ ìƒì„¸")
        df = pd.DataFrame(rows)
        st.dataframe(df, use_container_width=True, hide_index=True)

        # CSV ë‹¤ìš´ë¡œë“œ
        csv = df.to_csv(index=False).encode("utf-8-sig")
        st.download_button(
            "ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ",
            data=csv,
            file_name="eval_results.csv",
            mime="text/csv",
            use_container_width=True
        )

        st.caption(f"ì„¤ì • ìš”ì•½: k={k}, rerank={rerank}, cand_factor={cand_factor} â†’ ë¦¬ë­í¬ í›„ë³´ â‰ˆ {int(round(k * max(1.0, cand_factor)))}ê°œ")
        n_eval = len(ndcgs)
        st.caption(f"í‰ê°€ í‘œë³¸: {n_eval}/{n_total} (gold_ids ë³´ìœ )")
        
    with st.expander("ì§€í‘œ ì„¤ëª…"):
        st.markdown(f"""
- **nDCG@{int(k)}**: ìƒìœ„ {int(k)}ê°œ ìˆœìœ„ì—ì„œ ì •ë‹µì´ ì–¼ë§ˆë‚˜ ìœ„ì— ë°°ì¹˜ë˜ì—ˆëŠ”ì§€(ë¡œê·¸ í• ì¸). 1.0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ.
- **MRR@{int(k)}**: ì²« ì •ë‹µì˜ ì—­ìˆœìœ„ í‰ê· (1ë“±=1.0, 5ë“±=0.2). ì •ë‹µì„ ëª‡ ë²ˆì§¸ì—ì„œ ì°¾ëŠ”ì§€ì˜ ì§ê´€ ì§€í‘œ.
- **Recall@{int(k)}**: ìƒìœ„ {int(k)}ê°œ ì•ˆì— ì •ë‹µì´ í•œ ë²ˆì´ë¼ë„ í¬í•¨ë˜ì—ˆëŠ”ì§€(íšŒìˆ˜ìœ¨).
- **P95 latency**: ì „ì²´ ìš”ì²­ ì¤‘ 95%ê°€ ì´ ì‹œê°„ ì´ë‚´ì— ëë‚¬ìŒì„ ì˜ë¯¸(ìµœì•…ì— ê°€ê¹Œìš´ ì§€ì—°).
        """)
# =======================================================================
