#!/usr/bin/env python3
# kainexa-core/add_workflow_endpoint.py
# ê¸°ì¡´ Core APIì— ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
from datetime import datetime
import uvicorn

app = FastAPI(
    title="Kainexa Core API with Workflow",
    version="1.0.0",
    description="Core API with Workflow Execution"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ìš”ì²­/ì‘ë‹µ ëª¨ë¸
class WorkflowNode(BaseModel):
    id: str
    type: str
    position: Dict[str, float]
    data: Dict[str, Any]

class WorkflowEdge(BaseModel):
    id: str
    source: str
    target: str
    type: Optional[str] = "default"

class WorkflowExecuteRequest(BaseModel):
    nodes: List[Dict[str, Any]]
    edges: List[Dict[str, Any]]
    context: Optional[Dict[str, Any]] = {}

class WorkflowExecuteResponse(BaseModel):
    execution_id: str
    status: str
    message: str
    results: Optional[List[Dict[str, Any]]] = None
    timestamp: str

# ê¸°ì¡´ ì—”ë“œí¬ì¸íŠ¸
@app.get("/")
async def root():
    return {
        "name": "Kainexa AI Platform",
        "version": "1.0.0",
        "status": "operational",
        "endpoints": {
            "chat": "/api/v1/chat",
            "documents": "/api/v1/documents",
            "scenarios": "/api/v1/scenarios",
            "health": "/api/v1/health/full",
            "workflow": "/api/v1/workflow/execute"  # ì¶”ê°€
        }
    }

@app.get("/api/v1/health/full")
async def health_full():
    return {
        "status": "healthy",
        "services": {
            "llm": {"status": "healthy", "model": "solar"},
            "rag": {"status": "healthy"},
            "database": {"status": "healthy"},
            "cache": {"status": "healthy"}
        },
        "timestamp": datetime.now().isoformat()
    }

# âœ… ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€
@app.post("/api/v1/workflow/execute", response_model=WorkflowExecuteResponse)
async def execute_workflow(request: WorkflowExecuteRequest):
    """ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    
    print(f"\n{'='*50}")
    print(f"ğŸš€ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ìš”ì²­")
    print(f"{'='*50}")
    print(f"ë…¸ë“œ ìˆ˜: {len(request.nodes)}")
    print(f"ì—°ê²° ìˆ˜: {len(request.edges)}")
    
    # ë…¸ë“œ ì •ë³´ ì¶œë ¥
    results = []
    for i, node in enumerate(request.nodes):
        node_id = node.get('id', f'node_{i}')
        node_type = node.get('type', 'unknown')
        node_data = node.get('data', {})
        
        print(f"\nğŸ“Œ ë…¸ë“œ {i+1}: {node_id}")
        print(f"  íƒ€ì…: {node_type}")
        print(f"  ë¼ë²¨: {node_data.get('label', 'N/A')}")
        
        # ë…¸ë“œ íƒ€ì…ë³„ ì‹¤í–‰ ì‹œë®¬ë ˆì´ì…˜
        result = {
            "node_id": node_id,
            "type": node_type,
            "status": "executed",
            "output": None
        }
        
        if node_type == 'intent':
            result["output"] = {
                "intent": "greeting",
                "confidence": 0.95,
                "message": "ì˜ë„ ë¶„ë¥˜ ì™„ë£Œ: ì¸ì‚¬"
            }
        elif node_type == 'llm':
            result["output"] = {
                "response": "ì•ˆë…•í•˜ì„¸ìš”! Kainexa AIì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
                "model": node_data.get('config', {}).get('model', 'solar'),
                "tokens": 24
            }
        elif node_type == 'api':
            result["output"] = {
                "status_code": 200,
                "data": {"result": "API í˜¸ì¶œ ì„±ê³µ"},
                "url": node_data.get('config', {}).get('url', 'https://api.example.com')
            }
        elif node_type == 'condition':
            result["output"] = {
                "condition_met": True,
                "next_node": "node_2"
            }
        elif node_type == 'loop':
            result["output"] = {
                "iterations": 3,
                "completed": True
            }
        
        results.append(result)
    
    # ì‹¤í–‰ ì™„ë£Œ
    print(f"\nâœ… ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì™„ë£Œ!")
    print(f"{'='*50}")
    
    return WorkflowExecuteResponse(
        execution_id=f"exec_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        status="completed",
        message=f"ì›Œí¬í”Œë¡œìš°ê°€ ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰ë˜ì—ˆìŠµë‹ˆë‹¤. {len(request.nodes)}ê°œ ë…¸ë“œ ì²˜ë¦¬ ì™„ë£Œ.",
        results=results,
        timestamp=datetime.now().isoformat()
    )

# ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸
@app.post("/api/v1/chat")
async def chat(message: str = "ì•ˆë…•í•˜ì„¸ìš”"):
    """ì±„íŒ… ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
    return {
        "response": f"ì…ë ¥í•˜ì‹  '{message}'ì— ëŒ€í•œ AI ì‘ë‹µì…ë‹ˆë‹¤.",
        "session_id": "test-session",
        "timestamp": datetime.now().isoformat()
    }

# ì‹œë‚˜ë¦¬ì˜¤ ì—”ë“œí¬ì¸íŠ¸ë“¤
@app.post("/api/v1/scenarios/production")
async def production_scenario(query: str = "ìƒì‚° í˜„í™©"):
    """ìƒì‚° ëª¨ë‹ˆí„°ë§ ì‹œë‚˜ë¦¬ì˜¤"""
    return {
        "status": "success",
        "query": query,
        "data": {
            "total": {
                "planned": 1000,
                "actual": 950,
                "achievement_rate": 95.0,
                "defects": 10,
                "defect_rate": 1.05
            }
        },
        "report": f"{query}ì— ëŒ€í•œ ë¶„ì„: ìƒì‚° ë‹¬ì„±ë¥  95%, í’ˆì§ˆ ì–‘í˜¸",
        "timestamp": datetime.now().isoformat()
    }

if __name__ == "__main__":
    print("\n" + "="*60)
    print("ğŸš€ Kainexa Core API ì‹œì‘ (ì›Œí¬í”Œë¡œìš° ì§€ì›)")
    print("="*60)
    print("\nğŸ“¡ ì ‘ì† URL:")
    print("  - API: http://localhost:8000")
    print("  - API: http://192.168.1.215:8000")
    print("  - Docs: http://192.168.1.215:8000/docs")
    print("\nâœ… ì›Œí¬í”Œë¡œìš° ì‹¤í–‰: POST /api/v1/workflow/execute")
    print("="*60 + "\n")
    
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)