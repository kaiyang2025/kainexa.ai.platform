Kainexa AI Agent Platform - Ubuntu ì„œë²„ í™˜ê²½ ì„¤ì • ë° í…ŒìŠ¤íŠ¸ ê°€ì´ë“œ
ğŸ“‹ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ í™•ì¸

OS: Ubuntu 24.04.3 LTS
GPU: NVIDIA RTX 3090 Ã—2 (24GB Ã—2)
RAM: ìµœì†Œ 32GB ê¶Œì¥
Storage: ìµœì†Œ 100GB ì—¬ìœ  ê³µê°„


1ï¸âƒ£ ê¸°ë³¸ ì‹œìŠ¤í…œ ì„¤ì •
Step 1.1: ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸
bash# ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì—…ë°ì´íŠ¸
sudo apt update && sudo apt upgrade -y

# í•„ìˆ˜ ë„êµ¬ ì„¤ì¹˜
sudo apt install -y \
    build-essential \
    curl \
    wget \
    git \
    vim \
    htop \
    net-tools \
    software-properties-common \
    apt-transport-https \
    ca-certificates \
    gnupg \
    lsb-release
Step 1.2: Python 3.11 ì„¤ì¹˜
bash# Python 3.11 ì„¤ì¹˜
sudo add-apt-repository ppa:deadsnakes/ppa -y
sudo apt update
sudo apt install -y python3.11 python3.11-venv python3.11-dev python3-pip

# ê¸°ë³¸ Python ì„¤ì •
sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1
sudo update-alternatives --config python3

# pip ì—…ê·¸ë ˆì´ë“œ
python3 -m pip install --upgrade pip

2ï¸âƒ£ NVIDIA GPU ë“œë¼ì´ë²„ ë° CUDA ì„¤ì •
Step 2.1: NVIDIA ë“œë¼ì´ë²„ ì„¤ì¹˜
bash# ê¸°ì¡´ ë“œë¼ì´ë²„ ì œê±°
sudo apt-get purge nvidia* -y
sudo apt-get autoremove -y

# NVIDIA ë“œë¼ì´ë²„ ì„¤ì¹˜ (535 ë²„ì „ - RTX 3090 ì§€ì›)
sudo add-apt-repository ppa:graphics-drivers/ppa -y
sudo apt update
sudo apt install -y nvidia-driver-535

# ì¬ë¶€íŒ…
sudo reboot
Step 2.2: CUDA 11.8 ì„¤ì¹˜
bash# ì¬ë¶€íŒ… í›„ ë“œë¼ì´ë²„ í™•ì¸
nvidia-smi

# CUDA 11.8 ì„¤ì¹˜
wget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda_11.8.0_520.61.05_linux.run
sudo sh cuda_11.8.0_520.61.05_linux.run --toolkit --silent

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
echo 'export PATH=/usr/local/cuda-11.8/bin:$PATH' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=/usr/local/cuda-11.8/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
source ~/.bashrc

# @ CUDA 11.8 ì„¤ì¹˜ ì˜¤ë¥˜ ì‹œ  2ê°€ì§€ ë°©ë²• #####################################################
# Ubuntu 24.04ëŠ” GCC 13.3ì„ ì‚¬ìš©í•˜ëŠ”ë°, CUDA 11.8ì€ GCC 11ê¹Œì§€ë§Œ ê³µì‹ ì§€ì›
# 1

# 1. CUDA 12.3 ì„¤ì¹˜ ë‹¤ìš´ë¡œë“œ (GCC 13 ì§€ì›)
wget https://developer.download.nvidia.com/compute/cuda/12.3.2/local_installers/cuda_12.3.2_545.23.08_linux.run

# 2. ì„¤ì¹˜
sudo sh cuda_12.3.2_545.23.08_linux.run --toolkit --silent

# 3. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
echo 'export PATH=/usr/local/cuda-12.3/bin:$PATH' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=/usr/local/cuda-12.3/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
source ~/.bashrc

# 4. PyTorchëŠ” CUDA 12.1 ì§€ì›
pip install torch==2.1.0+cu121 torchvision==0.16.0+cu121 torchaudio==2.1.0+cu121 --index-url https://download.pytorch.org/whl/cu121


# 2

# GCC 11 ì„¤ì¹˜ ë° CUDA 11.8 ì„¤ì¹˜
# 1. GCC 11 ì„¤ì¹˜
sudo apt install gcc-11 g++-11 -y

# 2. CUDA ì„¤ì¹˜ ì‹œ GCC 11 ëª…ì‹œì  ì§€ì •
sudo sh cuda_11.8.0_520.61.05_linux.run \
    --toolkit \
    --override \
    --silent \
    --toolkitpath=/usr/local/cuda-11.8

# 3. ì„¤ì¹˜ í™•ì¸
ls /usr/local/cuda-11.8/

# 4. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
cat >> ~/.bashrc << 'EOF'
# CUDA 11.8
export PATH=/usr/local/cuda-11.8/bin:$PATH
export LD_LIBRARY_PATH=/usr/local/cuda-11.8/lib64:$LD_LIBRARY_PATH
export CUDA_HOME=/usr/local/cuda-11.8
EOF

source ~/.bashrc

# 5. CUDA ì„¤ì¹˜ í™•ì¸
nvcc --version
nvidia-smi

# ########################################################

# CUDA ì„¤ì¹˜ í™•ì¸
nvcc --version

Step 2.3: cuDNN ì„¤ì¹˜
bash# cuDNN 8.6 ë‹¤ìš´ë¡œë“œ (NVIDIA ê³„ì • í•„ìš”)
# https://developer.nvidia.com/cudnn ì—ì„œ ë‹¤ìš´ë¡œë“œ í›„

tar -xvf cudnn-linux-x86_64-8.6.0.163_cuda11-archive.tar.xz
sudo cp cudnn-*-archive/include/cudnn*.h /usr/local/cuda-11.8/include
sudo cp -P cudnn-*-archive/lib/libcudnn* /usr/local/cuda-11.8/lib64
sudo chmod a+r /usr/local/cuda-11.8/include/cudnn*.h /usr/local/cuda-11.8/lib64/libcudnn*


Step 2.3: cuDNN ì„¤ì¹˜ 2
# https://developer.nvidia.com/cudnn

wget https://developer.download.nvidia.com/compute/cudnn/9.13.0/local_installers/cudnn-local-repo-ubuntu2404-9.13.0_1.0-1_amd64.deb
sudo dpkg -i cudnn-local-repo-ubuntu2404-9.13.0_1.0-1_amd64.deb
sudo cp /var/cudnn-local-repo-ubuntu2404-9.13.0/cudnn-*-keyring.gpg /usr/share/keyrings/
sudo apt-get update
sudo apt-get -y install cudnn

To install for CUDA 12, perform the above configuration but install the CUDA 12 specific package:
sudo apt-get -y install cudnn9-cuda-12

Step 2.4: NVLink ì„¤ì • í™•ì¸
bash# NVLink ìƒíƒœ í™•ì¸
nvidia-smi nvlink -s
nvidia-smi topo -m

# P2P í†µì‹  í™œì„±í™”
sudo nvidia-smi -pm 1
sudo nvidia-smi -lgc 1695  # RTX 3090 ìµœëŒ€ í´ëŸ­

# ì „ë ¥ ì œí•œ ì„¤ì • (350W - RTX 3090 TDP)
sudo nvidia-smi -pl 350

# í˜„ì¬ ì„¤ì • í™•ì¸
nvidia-smi -q -d PERFORMANCE

3ï¸âƒ£ Docker ë° Docker Compose ì„¤ì¹˜
Step 3.1: Docker ì„¤ì¹˜
bash# Docker GPG í‚¤ ì¶”ê°€
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg

# Docker ì €ì¥ì†Œ ì¶”ê°€
echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

# Docker ì„¤ì¹˜
sudo apt update
sudo apt install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin

# ì‚¬ìš©ìë¥¼ docker ê·¸ë£¹ì— ì¶”ê°€
sudo usermod -aG docker $USER
newgrp docker

# Docker ì„œë¹„ìŠ¤ ì‹œì‘
sudo systemctl enable docker
sudo systemctl start docker

Step 3.2: NVIDIA Container Toolkit ì„¤ì¹˜

bash# NVIDIA Container Toolkit ì„¤ì¹˜
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt update
sudo apt install -y nvidia-container-toolkit
sudo systemctl restart docker

# GPU ì§€ì› í™•ì¸
docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi

4ï¸âƒ£ Kainexa í”„ë¡œì íŠ¸ ì„¤ì •
Step 4.1: í”„ë¡œì íŠ¸ í´ë¡  ë° ì„¤ì •
bash# í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p ~/kainexa
cd ~/kainexa

# Git í´ë¡  (ì‹¤ì œ ì €ì¥ì†Œ URLë¡œ ë³€ê²½)
git clone https://github.com/kaiyang2025/kainexa.ai.platform.git
cd kainexa-core

# í™˜ê²½ ì„¤ì • íŒŒì¼ ìƒì„±
cp .env.example .env

# .env íŒŒì¼ ìˆ˜ì •
vim .env
Step 4.2: .env íŒŒì¼ ì„¤ì •
bash# .env íŒŒì¼ ë‚´ìš©
APP_NAME="Kainexa Core"
APP_VERSION="0.1.0"
ENVIRONMENT="production"
DEBUG=False

# API
API_PREFIX="/api/v1"
ALLOWED_ORIGINS="http://localhost:3000,http://your-domain.com"

# Database
DATABASE_URL="postgresql+asyncpg://kainexa:your_password@localhost:5432/kainexa_db"
REDIS_URL="redis://localhost:6379"

# Qdrant
QDRANT_HOST="localhost"
QDRANT_PORT=6333

# GPU ì„¤ì •
CUDA_VISIBLE_DEVICES="0,1"
TENSOR_PARALLEL_SIZE=2

# Security
SECRET_KEY="change-this-to-secure-key-$(openssl rand -hex 32)"
ACCESS_TOKEN_EXPIRE_MINUTES=1440

# Model ê²½ë¡œ
MODEL_PATH="/models"
SOLAR_MODEL="upstage/solar-10.7b-instruct"


Step 4.3: Python ê°€ìƒí™˜ê²½ ì„¤ì •
bash# ê°€ìƒí™˜ê²½ ìƒì„±
python3.11 -m venv venv
source venv/bin/activate

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install --upgrade pip
pip install -r requirements.txt

# GPU ì˜ì¡´ì„± ì„¤ì¹˜
pip install torch==2.1.0+cu118 torchvision==0.16.0+cu118 torchaudio==2.1.0+cu118 --index-url https://download.pytorch.org/whl/cu118
pip install -r requirements-gpu.txt

5ï¸âƒ£ ë°ì´í„°ë² ì´ìŠ¤ ë° ì„œë¹„ìŠ¤ ì‹œì‘
Step 5.1: Docker ì„œë¹„ìŠ¤ ì‹œì‘
bash# Docker ì„œë¹„ìŠ¤ ì‹œì‘ (PostgreSQL, Redis, Qdrant)
make docker-up

# ë˜ëŠ” ì§ì ‘ ì‹¤í–‰
docker compose up -d postgres redis qdrant

# ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
docker ps
docker compose ps

Step 5.2: ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
bash# ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜
source venv/bin/activate
alembic upgrade head

# ë˜ëŠ” Makefile ì‚¬ìš©
make migrate

Step 5.3: ëª¨ë‹ˆí„°ë§ ìŠ¤íƒ ì‹œì‘
bash# Prometheus, Grafana, Jaeger ì‹œì‘
make monitor-up

# ë˜ëŠ” ì§ì ‘ ì‹¤í–‰
docker-compose -f docker-compose.monitoring.yml up -d

# ì ‘ì† í™•ì¸
# Grafana: http://localhost:3000 (admin/kainexa123)
# Prometheus: http://localhost:9090
# Jaeger: http://localhost:16686

6ï¸âƒ£ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì„¤ì •
Step 6.1: Solar ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
bash# ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±
sudo mkdir -p /models
sudo chown $USER:$USER /models

# Hugging Face CLI ì„¤ì¹˜
pip install huggingface-hub

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì•½ 20GB)
python3 << EOF
from huggingface_hub import snapshot_download
snapshot_download(
    repo_id="upstage/solar-10.7b-instruct",
    local_dir="/models/solar-10.7b",
    local_dir_use_symlinks=False
)
EOF
Step 6.2: ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸
bash# GPU ë©”ëª¨ë¦¬ ë° ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸
python3 << EOF
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

print(f"CUDA Available: {torch.cuda.is_available()}")
print(f"GPU Count: {torch.cuda.device_count()}")

# ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸ (ë©”ëª¨ë¦¬ ì²´í¬)
model_path = "/models/solar-10.7b"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float16,
    device_map="auto"
)
print("Model loaded successfully!")
EOF
# ############################
# 1. í™˜ê²½ ì¤€ë¹„
cd ~/kainexa.ai.platform/kainexa-core
source venv/bin/activate

# 2. Docker ì„œë¹„ìŠ¤ ì‹œì‘
docker compose up -d

# 3. ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
python scripts/init_database.py

# 4. Solar ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì²˜ìŒ í•œ ë²ˆë§Œ)
python scripts/download_solar_model.py --download

# 5. ìƒ˜í”Œ ë¬¸ì„œ ì—…ë¡œë“œ
python scripts/upload_documents.py

# 6. í†µí•© API ì„œë²„ ì‹œì‘
python src/api/main_integrated.py

# 7. ìƒˆ í„°ë¯¸ë„ì—ì„œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
python scripts/test_api.py

# 8. ê°œë³„ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
python src/scenarios/production_monitoring.py
python src/scenarios/predictive_maintenance.py
python src/scenarios/quality_control.py

# ##############################

7ï¸âƒ£ ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ë° í…ŒìŠ¤íŠ¸
Step 7.1: ê°œë°œ ëª¨ë“œ ì‹¤í–‰
bash# ê°œë°œ ì„œë²„ ì‹œì‘
make dev

# ë˜ëŠ” ì§ì ‘ ì‹¤í–‰
source venv/bin/activate
uvicorn src.api.main:app --reload --host 0.0.0.0 --port 8000
Step 7.2: GPU ë³‘ë ¬ ëª¨ë“œ ì‹¤í–‰
bash# í…ì„œ ë³‘ë ¬ ì‹¤í–‰ (2 GPU)
make gpu-run

# ë˜ëŠ” ì§ì ‘ ì‹¤í–‰
./scripts/run_gpu_parallel.sh parallel
Step 7.3: í”„ë¡œë•ì…˜ ëª¨ë“œ ì‹¤í–‰
bash# ì „ì²´ ìŠ¤íƒ ì‹œì‘
make full-stack

# ë˜ëŠ” ê°œë³„ ì‹¤í–‰
# 1. ì„œë¹„ìŠ¤ ì‹œì‘
docker-compose up -d

# 2. API ì„œë²„ ì‹œì‘ (Gunicorn)
gunicorn src.api.main:app \
    -w 4 \
    -k uvicorn.workers.UvicornWorker \
    --bind 0.0.0.0:8000 \
    --access-logfile logs/access.log \
    --error-logfile logs/error.log

8ï¸âƒ£ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
Step 8.1: í—¬ìŠ¤ì²´í¬
bash# API í—¬ìŠ¤ì²´í¬
curl http://localhost:8000/api/v1/health

# ìƒì„¸ í—¬ìŠ¤ì²´í¬
curl http://localhost:8000/api/v1/health/detailed
Step 8.2: í†µí•© í…ŒìŠ¤íŠ¸
bash# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
source venv/bin/activate
pytest tests/ -v

# íŠ¹ì • í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰
pytest tests/test_integration.py -v
Step 8.3: ì œì¡°ì—… ì‹œë‚˜ë¦¬ì˜¤ ë°ëª¨
bash# ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰
python src/scenarios/manufacturing_demo.py

# ë˜ëŠ” Makefile ì‚¬ìš©
make demo
Step 8.4: API í…ŒìŠ¤íŠ¸ (Postman/curl)
bash# 1. ì¸ì¦ í† í° íšë“
curl -X POST http://localhost:8000/api/v1/auth/login \
  -H "Content-Type: application/json" \
  -d '{"username":"admin","password":"admin123"}'

# 2. ì±„íŒ… API í…ŒìŠ¤íŠ¸
TOKEN="your-token-here"
curl -X POST http://localhost:8000/api/v1/chat \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"message":"ì§€ë‚œë‹¬ ë§¤ì¶œ í˜„í™© ì•Œë ¤ì¤˜","session_id":"test-001"}'

# 3. WebSocket í…ŒìŠ¤íŠ¸
wscat -c ws://localhost:8000/api/v1/conversations/test-001/ws \
  -H "Authorization: Bearer $TOKEN"
Step 8.5: ë¶€í•˜ í…ŒìŠ¤íŠ¸
bash# Locust ì„¤ì¹˜
pip install locust

# ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
locust -f tests/load_test.py --host=http://localhost:8000
# ë¸Œë¼ìš°ì €ì—ì„œ http://localhost:8089 ì ‘ì†

9ï¸âƒ£ ëª¨ë‹ˆí„°ë§ ë° ë¡œê·¸ í™•ì¸
Step 9.1: ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§
bash# GPU ëª¨ë‹ˆí„°ë§
watch -n 1 nvidia-smi

# ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
htop

# Docker ì»¨í…Œì´ë„ˆ ëª¨ë‹ˆí„°ë§
docker stats
Step 9.2: ë¡œê·¸ í™•ì¸
bash# API ë¡œê·¸
tail -f logs/app_*.log

# Docker ë¡œê·¸
docker-compose logs -f api
docker-compose logs -f postgres

# ì‹œìŠ¤í…œ ë¡œê·¸
journalctl -f -u docker
Step 9.3: ë©”íŠ¸ë¦­ ëŒ€ì‹œë³´ë“œ
bash# Grafana ì ‘ì†
firefox http://localhost:3000
# Login: admin / kainexa123

# Prometheus ë©”íŠ¸ë¦­ í™•ì¸
firefox http://localhost:9090

# Jaeger íŠ¸ë ˆì´ì‹±
firefox http://localhost:16686

ğŸ”Ÿ ë¬¸ì œ í•´ê²° (Troubleshooting)
GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
bash# GPU ë©”ëª¨ë¦¬ ì •ë¦¬
sudo nvidia-smi --gpu-reset

# í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
sudo fuser -v /dev/nvidia*
sudo kill -9 <PID>
Docker ê¶Œí•œ ë¬¸ì œ
bash# Docker ì†Œì¼“ ê¶Œí•œ
sudo chmod 666 /var/run/docker.sock

# Docker ì¬ì‹œì‘
sudo systemctl restart docker
ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨
bash# ìºì‹œ ì •ë¦¬
rm -rf ~/.cache/huggingface/

# ëª¨ë¸ ì¬ë‹¤ìš´ë¡œë“œ
python -c "from transformers import AutoModel; AutoModel.from_pretrained('upstage/solar-10.7b-instruct', force_download=True)"

âœ… ìµœì¢… í™•ì¸ ì²´í¬ë¦¬ìŠ¤íŠ¸
bash# ì‹œìŠ¤í…œ ìƒíƒœ ì¢…í•© í™•ì¸
make status

# ë˜ëŠ” ìˆ˜ë™ í™•ì¸
echo "=== System Check ==="
echo "1. GPU Status:"
nvidia-smi
echo ""
echo "2. Docker Services:"
docker-compose ps
echo ""
echo "3. API Health:"
curl -s http://localhost:8000/api/v1/health | jq
echo ""
echo "4. Database Connection:"
docker exec kainexa-postgres pg_isready -U kainexa
echo ""
echo "5. Redis Connection:"
docker exec kainexa-redis redis-cli ping
echo ""
echo "6. Qdrant Status:"
curl -s http://localhost:6333/health
ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
bash# GPU ë²¤ì¹˜ë§ˆí¬
make gpu-benchmark

# ì¶”ë¡  ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
python3 tests/benchmark/inference_benchmark.py

# ì˜ˆìƒ ê²°ê³¼ (RTX 3090 x2, Solar-10.7B)
# - ë‹¨ì¼ GPU: ~15-20 tokens/sec
# - í…ì„œ ë³‘ë ¬: ~25-35 tokens/sec
# - ì²« í† í° ì§€ì—°: ~2-3ì´ˆ