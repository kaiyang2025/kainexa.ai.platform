# src/core/orchestration/step_executors.py
"""Î™®Îì† Step Executor ÌÜµÌï©"""
from typing import Dict, Any, List, Optional
from abc import ABC, abstractmethod
import asyncio
from datetime import datetime
import structlog

from src.nlp.korean_nlp import KoreanNLPPipeline
from src.core.governance.vector_store import VectorStore
from src.core.models.model_factory import ModelFactory

logger = structlog.get_logger()

class BaseExecutor(ABC):
    """Ïã§ÌñâÏûê Î≤†Ïù¥Ïä§ ÌÅ¥ÎûòÏä§"""
    
    @abstractmethod
    async def execute(self, context: Dict[str, Any]) -> Any:
        pass

class IntentClassifyExecutor(BaseExecutor):
    """ÏùòÎèÑ Î∂ÑÎ•ò Ïã§ÌñâÏûê"""
    
    def __init__(self):
        self.nlp_pipeline = KoreanNLPPipeline()
    
    async def execute(self, context: Dict[str, Any]) -> Dict:
        text = context.get('input_text', '')
        result = await self.nlp_pipeline.process(text)
        
        return {
            'intent': result.intent,
            'entities': result.entities,
            'honorific_level': result.honorific_level.value,
            'sentiment': result.sentiment
        }

class RetrieveKnowledgeExecutor(BaseExecutor):
    """ÏßÄÏãù Í≤ÄÏÉâ Ïã§ÌñâÏûê"""
    
    def __init__(self):
        self.vector_store = VectorStore()
    
    async def execute(self, context: Dict[str, Any]) -> List[Dict]:
        query = context.get('query', context.get('input_text', ''))
        k = context.get('k', 5)
        
        results = await self.vector_store.search(query, k)
        
        # Re-ranking Ï†ÅÏö©
        if len(results) > 1:
            results = self._rerank(query, results)
        
        return results
    
    def _rerank(self, query: str, results: List[Dict]) -> List[Dict]:
        """Í≤∞Í≥º Ïû¨ÏàúÏúÑÌôî"""
        # Cross-encoder ÏÇ¨Ïö© (Í∞ÑÎã® Íµ¨ÌòÑ)
        for result in results:
            # ÏøºÎ¶¨ÏôÄ ÌÖçÏä§Ìä∏ Ïú†ÏÇ¨ÎèÑ Ïû¨Í≥ÑÏÇ∞
            result['rerank_score'] = result.get('score', 0.0) * 0.8
        
        return sorted(results, key=lambda x: x['rerank_score'], reverse=True)

class LLMGenerateExecutor(BaseExecutor):
    """LLM ÏÉùÏÑ± Ïã§ÌñâÏûê"""
    
    def __init__(self):
        self.model_factory = ModelFactory()
        self.llm = None
    
    async def execute(self, context: Dict[str, Any]) -> str:
        # Î™®Îç∏ Ï¥àÍ∏∞Ìôî (lazy loading)
        if not self.llm:
            model_type = context.get('model', 'solar')
            self.llm = self.model_factory.create_model(model_type)
        
        # ÌîÑÎ°¨ÌîÑÌä∏ Íµ¨ÏÑ±
        prompt = self._build_prompt(context)
        
        # ÏÉùÏÑ± ÌååÎùºÎØ∏ÌÑ∞
        params = {
            'temperature': context.get('temperature', 0.7),
            'max_tokens': context.get('max_tokens', 512),
            'top_p': context.get('top_p', 0.9)
        }
        
        # LLM ÏÉùÏÑ±
        response = await self.llm.generate(prompt, **params)
        
        return response

    def _build_prompt(self, context: Dict[str, Any]) -> str:
        """ÌîÑÎ°¨ÌîÑÌä∏ Íµ¨ÏÑ±"""
        template = context.get('prompt_template', '')
        
        # Í∏∞Î≥∏ ÌÖúÌîåÎ¶ø
        if not template:
            template = """ÎãπÏã†ÏùÄ ÎèÑÏõÄÏù¥ ÎêòÎäî AI Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏ÏûÖÎãàÎã§.
            
ÏÇ¨Ïö©Ïûê: {input_text}

{knowledge}

ÏùëÎãµ:"""
        
        # Ïª®ÌÖçÏä§Ìä∏ Î≥ÄÏàòÎ°ú ÏπòÌôò
        prompt = template
        
        # ÏûÖÎ†• ÌÖçÏä§Ìä∏
        prompt = prompt.replace('{input_text}', context.get('input_text', ''))
        
        # RAG Í≤∞Í≥º Ï∂îÍ∞Ä
        if 'retrieved_knowledge' in context:
            knowledge_text = "\nÍ¥ÄÎ†® Ï†ïÎ≥¥:\n"
            for i, doc in enumerate(context['retrieved_knowledge'][:3], 1):
                knowledge_text += f"{i}. {doc.get('text', '')}\n"
            prompt = prompt.replace('{knowledge}', knowledge_text)
        else:
            prompt = prompt.replace('{knowledge}', '')
        
        # ÎåÄÌôî Ïù¥Î†• Ï∂îÍ∞Ä
        if 'conversation_history' in context:
            history = context['conversation_history'][-5:]  # ÏµúÍ∑º 5Í∞ú
            history_text = "\nÏù¥Ï†Ñ ÎåÄÌôî:\n"
            for h in history:
                history_text += f"ÏÇ¨Ïö©Ïûê: {h.get('user', '')}\n"
                history_text += f"AI: {h.get('assistant', '')}\n"
            prompt = history_text + "\n" + prompt
        
        return prompt

class MCPExecutionExecutor(BaseExecutor):
    """MCP Ïï°ÏÖò Ïã§ÌñâÏûê"""
    
    def __init__(self):
        self.action_registry = self._init_actions()
    
    def _init_actions(self) -> Dict:
        """Ïï°ÏÖò Î†àÏßÄÏä§Ìä∏Î¶¨ Ï¥àÍ∏∞Ìôî"""
        return {
            'send_email': self._send_email,
            'create_ticket': self._create_ticket,
            'update_database': self._update_database,
            'call_api': self._call_external_api
        }
    
    async def execute(self, context: Dict[str, Any]) -> Dict:
        action_type = context.get('action_type')
        
        if not action_type:
            return {'status': 'skipped', 'reason': 'No action required'}
        
        # Í∂åÌïú ÌôïÏù∏
        if not self._check_permission(context):
            return {'status': 'denied', 'reason': 'Insufficient permissions'}
        
        # Ïï°ÏÖò Ïã§Ìñâ
        if action_type in self.action_registry:
            result = await self.action_registry[action_type](context)
            return result
        
        return {'status': 'error', 'reason': f'Unknown action: {action_type}'}
    
    def _check_permission(self, context: Dict) -> bool:
        """Í∂åÌïú ÌôïÏù∏"""
        user_role = context.get('user_role', 'user')
        action_type = context.get('action_type')
        
        # Í∞ÑÎã®Ìïú Í∂åÌïú Îß§Ìä∏Î¶≠Ïä§
        permissions = {
            'user': [],
            'agent': ['send_email', 'create_ticket'],
            'admin': ['send_email', 'create_ticket', 'update_database', 'call_api']
        }
        
        return action_type in permissions.get(user_role, [])
    
    async def _send_email(self, context: Dict) -> Dict:
        """Ïù¥Î©îÏùº Î∞úÏÜ°"""
        logger.info("Sending email", context=context)
        # Ïã§Ï†ú Ïù¥Î©îÏùº Î∞úÏÜ° Î°úÏßÅ
        await asyncio.sleep(0.5)  # ÏãúÎÆ¨Î†àÏù¥ÏÖò
        return {'status': 'sent', 'message_id': 'email_12345'}
    
    async def _create_ticket(self, context: Dict) -> Dict:
        """Ìã∞Ïºì ÏÉùÏÑ±"""
        logger.info("Creating ticket", context=context)
        # Ìã∞Ïºì ÏãúÏä§ÌÖú Ïó∞Îèô
        return {'status': 'created', 'ticket_id': 'TICK-001'}
    
    async def _update_database(self, context: Dict) -> Dict:
        """Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ ÏóÖÎç∞Ïù¥Ìä∏"""
        logger.info("Updating database", context=context)
        # DB ÏóÖÎç∞Ïù¥Ìä∏ Î°úÏßÅ
        return {'status': 'updated', 'affected_rows': 1}
    
    async def _call_external_api(self, context: Dict) -> Dict:
        """Ïô∏Î∂Ä API Ìò∏Ï∂ú"""
        import httpx
        
        url = context.get('api_url')
        method = context.get('api_method', 'GET')
        payload = context.get('api_payload', {})
        
        async with httpx.AsyncClient() as client:
            response = await client.request(method, url, json=payload)
            
        return {
            'status': 'completed',
            'status_code': response.status_code,
            'response': response.json() if response.status_code == 200 else None
        }

class ResponsePostprocessExecutor(BaseExecutor):
    """ÏùëÎãµ ÌõÑÏ≤òÎ¶¨ Ïã§ÌñâÏûê"""
    
    def __init__(self):
        self.nlp_pipeline = KoreanNLPPipeline()
    
    async def execute(self, context: Dict[str, Any]) -> str:
        response = context.get('llm_response', '')
        
        # 1. Ï°¥ÎåìÎßê Î≥ÄÌôò
        target_honorific = context.get('target_honorific')
        if target_honorific:
            nlp_result = await self.nlp_pipeline.process(response, target_honorific)
            response = nlp_result.text
        
        # 2. Ï∂úÏ≤ò Ï∂îÍ∞Ä
        if 'retrieved_knowledge' in context:
            sources = []
            for doc in context['retrieved_knowledge'][:3]:
                source = doc.get('metadata', {}).get('source', 'Unknown')
                if source not in sources:
                    sources.append(source)
            
            if sources:
                response += f"\n\nÏ∂úÏ≤ò: {', '.join(sources)}"
        
        # 3. Ìè¨Îß∑ÌåÖ
        response = self._format_response(response)
        
        return response
    
    def _format_response(self, text: str) -> str:
        """ÏùëÎãµ Ìè¨Îß∑ÌåÖ"""
        # ÎßàÌÅ¨Îã§Ïö¥ Ï≤òÎ¶¨
        text = text.strip()
        
        # Ïù¥Î™®ÏßÄ Ï∂îÍ∞Ä (ÏÑ†ÌÉùÏ†Å)
        if 'Ï£ÑÏÜ°' in text:
            text = 'üòî ' + text
        elif 'Í∞êÏÇ¨' in text:
            text = 'üòä ' + text
        
        return text

# ---- Backward-compat shims for older tests ---
class IntentExecutor(IntentClassifyExecutor):
    """BC shim: old name kept for tests expecting `IntentExecutor`."""
    pass

# ---- Backward-compat shims for older tests ---
class IntentExecutor(IntentClassifyExecutor):
    """BC shim: old name kept for tests expecting `IntentExecutor`."""
    pass

class LLMExecutor(LLMGenerateExecutor):
    """BC shim: old name kept for tests expecting `LLMExecutor`."""
    pass

class KnowledgeExecutor(RetrieveKnowledgeExecutor):
    """BC shim: sometimes used instead of `RetrieveKnowledgeExecutor`."""
    pass

class ToolExecutor(MCPExecutionExecutor):
    """BC shim: some tests used `ToolExecutor` for MCP/external tools."""
    pass

class PostprocessExecutor(ResponsePostprocessExecutor):
    """BC shim: alias for response post-processing step."""
    pass


# Executor Î†àÏßÄÏä§Ìä∏Î¶¨
EXECUTOR_REGISTRY = {
    'intent_classify': IntentClassifyExecutor,   
    'retrieve_knowledge': RetrieveKnowledgeExecutor,
    'llm_generate': LLMGenerateExecutor,
    'mcp_execution': MCPExecutionExecutor,
    'response_postprocess': ResponsePostprocessExecutor,
    # backward-compat step type aliases (old DSL/test names)
    "intent": IntentExecutor,
    "llm": LLMExecutor,
    "knowledge": KnowledgeExecutor,
    "tool": ToolExecutor,
    "postprocess": PostprocessExecutor,
}

def create_executor(step_type: str) -> BaseExecutor:
    """Ïã§ÌñâÏûê ÏÉùÏÑ± Ìå©ÌÜ†Î¶¨"""
    executor_class = EXECUTOR_REGISTRY.get(step_type)
    if not executor_class:
        raise ValueError(f"Unknown executor type: {step_type}")
    return executor_class()

__all__ = [
    "BaseExecutor",
    "IntentClassifyExecutor",
    "RetrieveKnowledgeExecutor",
    "LLMGenerateExecutor",
    "MCPExecutionExecutor",
    "ResponsePostprocessExecutor",
    # Backward-compat symbols
    "IntentExecutor",
    "LLMExecutor",
    "KnowledgeExecutor",
    "ToolExecutor",
    "PostprocessExecutor",
]


