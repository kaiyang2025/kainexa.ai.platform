# kainexa-core/src/api/main_integrated.py ìˆ˜ì •
# Solar LLMì„ ì‹¤ì œë¡œ í˜¸ì¶œí•˜ëŠ” ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì½”ë“œ

from fastapi import FastAPI, HTTPException
from datetime import datetime, timezone
from uuid import uuid4
from typing import List, Dict, Any
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
import logging
import asyncio

from src.api.routes.integrated import router as integrated_router
from src.core.config import settings
from src.models.solar_llm import SolarLLM

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    logger.info("Starting Kainexa Integrated API")
    
    # LLM ì‹±ê¸€í„´ ì˜ˆì—´ & ìºì‹±
    try:
        if not hasattr(app.state, "llm") or app.state.llm is None:
            logger.info("Loading Solar LLM model...")
            
            # ëª¨ë¸ ê²½ë¡œë¥¼ ëª…í™•íˆ ì§€ì •
            model_path = os.getenv("SOLAR_MODEL_PATH", "beomi/OPEN-SOLAR-KO-10.7B")
            
            # ë¡œì»¬ ê²½ë¡œì¸ì§€ HuggingFace ëª¨ë¸ì¸ì§€ í™•ì¸
            if os.path.exists(model_path):
                # ë¡œì»¬ ëª¨ë¸
                logger.info(f"Using local model: {model_path}")
            else:
                # HuggingFace Hub ëª¨ë¸
                logger.info(f"Using HuggingFace model: {model_path}")
            
            llm = SolarLLM(
                model_path=model_path,  # ë¬¸ìì—´ë¡œ ì „ë‹¬
                load_in_8bit=True,  # 8ë¹„íŠ¸ ì–‘ìí™”
                device_map="auto"  # ìë™ ë¶„ì‚°
            )
            llm.load()
            app.state.llm = llm
            logger.info("âœ… SolarLLM loaded and cached successfully")
    except Exception as e:
        logger.exception("âŒ LLM warm-up failed: %s", e)
        app.state.llm = None
    
    yield
    
    # Shutdown
    logger.info("Shutting down Kainexa Integrated API")
    try:
        if hasattr(app.state, "llm") and app.state.llm is not None:
            app.state.llm = None
            logger.info("LLM resources released")
    except Exception:
        pass

app = FastAPI(
    title="Kainexa AI Platform",
    version="1.0.0",
    description="Manufacturing AI Agent Platform",
    lifespan=lifespan
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ë¼ìš°í„° í¬í•¨
app.include_router(integrated_router)

@app.get("/")
async def root():
    return {
        "name": "Kainexa AI Platform",
        "version": "1.0.0",
        "status": "operational",
        "endpoints": {
            "chat": "/api/v1/chat",
            "documents": "/api/v1/documents",
            "scenarios": "/api/v1/scenarios",
            "workflow": "/api/v1/workflow/execute",
            "health": "/api/v1/health/full"
        }
    }

@app.post("/api/v1/workflow/execute")
async def execute_workflow(request: dict):
    """ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ - ì‹¤ì œ LLM í˜¸ì¶œ í¬í•¨"""
    nodes = request.get("nodes", [])
    edges = request.get("edges", [])
    
    print(f"\n{'='*60}")
    print(f"ğŸš€ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹œì‘")
    print(f"{'='*60}")
    print(f"ğŸ“Š ë…¸ë“œ ìˆ˜: {len(nodes)}, ì—°ê²° ìˆ˜: {len(edges)}")
    
    results = []
    execution_log = []
    
    # ë…¸ë“œë³„ ì‹¤í–‰
    for i, node in enumerate(nodes):
        node_id = node.get('id', f'node_{i}')
        node_type = node.get('type', 'unknown')
        node_data = node.get('data', {})
        node_config = node_data.get('config', {})
        
        print(f"\n{'â”€'*40}")
        print(f"ğŸ“Œ ë…¸ë“œ {i+1}/{len(nodes)}: {node_id}")
        print(f"   íƒ€ì…: {node_type}")
        print(f"   ë¼ë²¨: {node_data.get('label', 'N/A')}")
        
        result = {
            "node_id": node_id,
            "type": node_type,
            "label": node_data.get('label'),
            "status": "executing",
            "output": None,
            "execution_time": None
        }
        
        start_time = datetime.now()
        
        try:
            # ë…¸ë“œ íƒ€ì…ë³„ ì‹¤í–‰
            if node_type == 'intent':
                # ì˜ë„ ë¶„ë¥˜ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­)
                print(f"   ğŸ¯ ì˜ë„ ë¶„ë¥˜ ì‹¤í–‰...")
                await asyncio.sleep(0.5)  # ì‹œë®¬ë ˆì´ì…˜ ë”œë ˆì´
                result["output"] = {
                    "intent": "greeting",
                    "confidence": 0.95,
                    "message": "ì‚¬ìš©ì ì˜ë„: ì¸ì‚¬/í™˜ì˜"
                }
                print(f"   âœ… ì˜ë„ ë¶„ë¥˜ ì™„ë£Œ: greeting (95% ì‹ ë¢°ë„)")
                
            elif node_type == 'llm':
                # â­ ì‹¤ì œ Solar LLM í˜¸ì¶œ
                print(f"   ğŸ¤– Solar LLM í˜¸ì¶œ ì¤‘...")
                
                # LLMì´ ë¡œë“œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                if not hasattr(app.state, "llm") or app.state.llm is None:
                    print(f"   âš ï¸  LLMì´ ë¡œë“œë˜ì§€ ì•ŠìŒ. Mock ì‘ë‹µ ì‚¬ìš©.")
                    result["output"] = {
                        "response": "[Mock] ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
                        "model": "mock",
                        "tokens": 0
                    }
                else:
                    try:
                        # ëª…í™•í•œ í•œêµ­ì–´ ì§€ì‹œ í”„ë¡¬í”„íŠ¸
                        system_prompt = """ë‹¹ì‹ ì€ í•œêµ­ ì œì¡°ì—… ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
                    ë°˜ë“œì‹œ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.
                    ì˜ì–´, í•œì, íŠ¹ìˆ˜ë¬¸ìë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
                    ì „ë¬¸ìš©ì–´ëŠ” í•œêµ­ì–´ë¡œ ì„¤ëª…í•˜ë˜, ì•½ì–´(OEE/IoT/AI)ëŠ” ê´„í˜¸ë¡œ ë³‘ê¸° ê°€ëŠ¥í•©ë‹ˆë‹¤."""
                        
                        # ì´ì „ ë…¸ë“œì˜ ì¶œë ¥ì„ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
                        user_message = "ì•ˆë…•í•˜ì„¸ìš”"
                        if i > 0 and results:
                            prev_output = results[-1].get('output', {})
                            if 'message' in prev_output:
                                user_message = prev_output['message']
                        
                        prompt = f"{system_prompt}\n\nì‚¬ìš©ì: {user_message}\nì–´ì‹œìŠ¤í„´íŠ¸:"
                        
                        print(f"   ğŸ“ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)}ì")
                        print(f"   â³ LLM ìƒì„± ì¤‘... (ìµœëŒ€ 30ì´ˆ ì†Œìš”)")
                        
                        # ì‹¤ì œ LLM í˜¸ì¶œ
                        response = app.state.llm.generate(
                            prompt=prompt,
                            max_new_tokens=100,
                            temperature=0.7,  # ì ì ˆí•œ ì°½ì˜ì„±
                            top_p=0.9,
                            top_k=50,
                            do_sample=True,  # ìƒ˜í”Œë§ í™œì„±í™”
                            ko_only=True,  # í•œêµ­ì–´ ì „ìš© ëª¨ë“œ
                        )
                        
                        result["output"] = {
                            "response": response,
                            "model": "solar-10.7b",
                            "tokens": len(response.split()),
                            "temperature": node_config.get('temperature', 0.7)
                        }
                        print(f"   âœ… LLM ì‘ë‹µ ìƒì„± ì™„ë£Œ ({len(response)}ì)")
                        print(f"   ğŸ’¬ ì‘ë‹µ: {response[:100]}...")
                        
                    except Exception as e:
                        print(f"   âŒ LLM í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
                        result["output"] = {
                            "response": f"LLM ì˜¤ë¥˜: {str(e)}",
                            "model": "solar-error",
                            "error": str(e)
                        }
                
            elif node_type == 'api':
                # API í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜
                print(f"   ğŸŒ API í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜...")
                url = node_config.get('url', 'https://api.example.com')
                await asyncio.sleep(0.3)
                result["output"] = {
                    "status_code": 200,
                    "url": url,
                    "data": {"result": "API ì‘ë‹µ ë°ì´í„°"}
                }
                print(f"   âœ… API í˜¸ì¶œ ì™„ë£Œ: {url}")
                
            elif node_type == 'condition':
                # ì¡°ê±´ ë¶„ê¸°
                print(f"   ğŸ”€ ì¡°ê±´ í‰ê°€...")
                result["output"] = {
                    "condition_met": True,
                    "expression": node_config.get('expression', 'true'),
                    "next_node": edges[0]['target'] if edges else None
                }
                print(f"   âœ… ì¡°ê±´ í‰ê°€ ì™„ë£Œ: True")
                
            elif node_type == 'loop':
                # ë°˜ë³µ ì‹¤í–‰
                iterations = node_config.get('iterations', 3)
                print(f"   ğŸ”„ ë°˜ë³µ ì‹¤í–‰ ({iterations}íšŒ)...")
                result["output"] = {
                    "iterations": iterations,
                    "completed": True,
                    "results": [f"iteration_{j+1}" for j in range(iterations)]
                }
                print(f"   âœ… ë°˜ë³µ ì™„ë£Œ: {iterations}íšŒ")
            
            else:
                result["output"] = {"message": f"Unknown node type: {node_type}"}
            
            result["status"] = "completed"
            
        except Exception as e:
            print(f"   âŒ ë…¸ë“œ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
            result["status"] = "error"
            result["output"] = {"error": str(e)}
        
        # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
        end_time = datetime.now()
        result["execution_time"] = (end_time - start_time).total_seconds()
        print(f"   â±ï¸  ì‹¤í–‰ ì‹œê°„: {result['execution_time']:.2f}ì´ˆ")
        
        results.append(result)
        execution_log.append(f"[{datetime.now().isoformat()}] {node_id}: {result['status']}")
    
    # ì „ì²´ ì‹¤í–‰ ì™„ë£Œ
    print(f"\n{'='*60}")
    print(f"âœ… ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì™„ë£Œ!")
    print(f"{'='*60}")
    print(f"ğŸ“Š ì‹¤í–‰ ê²°ê³¼:")
    print(f"   - ì„±ê³µ: {sum(1 for r in results if r['status'] == 'completed')}ê°œ")
    print(f"   - ì‹¤íŒ¨: {sum(1 for r in results if r['status'] == 'error')}ê°œ")
    print(f"   - ì´ ì‹¤í–‰ ì‹œê°„: {sum(r.get('execution_time', 0) for r in results):.2f}ì´ˆ")
    
    return {
        "execution_id": f"exec_{uuid4().hex[:8]}",
        "status": "completed",
        "message": f"ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì™„ë£Œ: {len(nodes)}ê°œ ë…¸ë“œ ì²˜ë¦¬",
        "nodes_executed": len(nodes),
        "results": results,
        "execution_log": execution_log,
        "timestamp": datetime.now(timezone.utc).isoformat()
    }

# LLM í…ŒìŠ¤íŠ¸ ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€
@app.post("/api/v1/test/llm")
async def test_llm(prompt: str = "ì•ˆë…•í•˜ì„¸ìš”. ìê¸°ì†Œê°œë¥¼ í•´ì£¼ì„¸ìš”."):
    """Solar LLM ë‹¨ë… í…ŒìŠ¤íŠ¸"""
    print(f"\n{'='*60}")
    print(f"ğŸ§ª Solar LLM í…ŒìŠ¤íŠ¸")
    print(f"{'='*60}")
    
    if not hasattr(app.state, "llm") or app.state.llm is None:
        return {
            "status": "error",
            "message": "LLM not loaded",
            "suggestion": "ì„œë²„ë¥¼ ì¬ì‹œì‘í•˜ì—¬ LLMì„ ë¡œë“œí•˜ì„¸ìš”"
        }
    
    try:
        print(f"ğŸ“ ì…ë ¥: {prompt}")
        print(f"â³ ìƒì„± ì¤‘...")
        
        start_time = datetime.now()
        response = app.state.llm.generate(
            prompt=prompt,
            max_new_tokens=150,
            temperature=0.7,
            do_sample=True
        )
        end_time = datetime.now()
        
        execution_time = (end_time - start_time).total_seconds()
        
        print(f"âœ… ìƒì„± ì™„ë£Œ ({execution_time:.2f}ì´ˆ)")
        print(f"ğŸ’¬ ì‘ë‹µ: {response[:200]}...")
        
        return {
            "status": "success",
            "prompt": prompt,
            "response": response,
            "model": "solar-10.7b",
            "execution_time": execution_time,
            "tokens": len(response.split()),
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        print(f"âŒ LLM í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        return {
            "status": "error",
            "message": str(e),
            "prompt": prompt
        }

# í—¬ìŠ¤ ì²´í¬
@app.get("/api/v1/health/full")
async def health_full():
    """ìƒì„¸ í—¬ìŠ¤ ì²´í¬"""
    llm_status = "healthy" if (hasattr(app.state, "llm") and app.state.llm is not None) else "not_loaded"
    
    return {
        "status": "healthy",
        "services": {
            "llm": {
                "status": llm_status,
                "model": "solar-10.7b" if llm_status == "healthy" else None
            },
            "rag": {"status": "healthy"},
            "database": {"status": "healthy"},
            "cache": {"status": "healthy"}
        },
        "timestamp": datetime.now(timezone.utc).isoformat()
    }

if __name__ == "__main__":
    import uvicorn
    print("\n" + "="*60)
    print("ğŸš€ Kainexa Core API with Solar LLM")
    print("="*60)
    print("ğŸ“¡ Endpoints:")
    print("  - Workflow: POST /api/v1/workflow/execute")
    print("  - LLM Test: POST /api/v1/test/llm")
    print("  - Health: GET /api/v1/health/full")
    print("="*60 + "\n")
    uvicorn.run(app, host="0.0.0.0", port=8000)