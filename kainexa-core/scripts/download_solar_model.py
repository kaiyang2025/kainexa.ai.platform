# scripts/download_solar_model.py ìƒì„±
#!/usr/bin/env python3
"""
Solar-10.7B ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì„¤ì •
"""
import os
import sys
import torch
from pathlib import Path
from huggingface_hub import snapshot_download
from transformers import AutoModelForCausalLM, AutoTokenizer

def download_solar_model():
    """Solar-10.7B ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
    
    model_id = "upstage/SOLAR-10.7B-Instruct-v1.0"  #upstage/solar-10.7b-instruct
    cache_dir = Path("models/solar-10.7b")
    
    print("="*60)
    print("Solar-10.7B ëª¨ë¸ ë‹¤ìš´ë¡œë“œ")
    print("="*60)
    print(f"ëª¨ë¸: {model_id}")
    print(f"ì €ì¥ ê²½ë¡œ: {cache_dir}")
    print("")
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    cache_dir.mkdir(parents=True, exist_ok=True)
    
    try:
        # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì•½ 20GB)
        print("â¬‡ï¸  ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘... (ì•½ 20GB, ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤)")
        
        # HuggingFace Hubì—ì„œ ë‹¤ìš´ë¡œë“œ
        snapshot_download(
            repo_id=model_id,
            local_dir=cache_dir,
            local_dir_use_symlinks=False,
            resume_download=True,  # ì¤‘ë‹¨ëœ ë‹¤ìš´ë¡œë“œ ì¬ê°œ
            ignore_patterns=["*.onnx", "*.msgpack"]  # ë¶ˆí•„ìš”í•œ íŒŒì¼ ì œì™¸
        )
        
        print("âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
        
        # ëª¨ë¸ íŒŒì¼ í™•ì¸
        model_files = list(cache_dir.glob("*.safetensors"))
        if model_files:
            total_size = sum(f.stat().st_size for f in model_files) / (1024**3)
            print(f"ğŸ“¦ ëª¨ë¸ í¬ê¸°: {total_size:.2f} GB")
        
        return True
        
    except Exception as e:
        print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False

def test_model_loading():
    """ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸"""
    
    print("\n" + "="*60)
    print("ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    model_path = Path("models/solar-10.7b")
    
    if not model_path.exists():
        print("âŒ ëª¨ë¸ì´ ë‹¤ìš´ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ë‹¤ìš´ë¡œë“œë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return False
    
    try:
        print("ğŸ”„ í† í¬ë‚˜ì´ì € ë¡œë”©...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        print("âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ")
        
        print("\nğŸ”„ ëª¨ë¸ ë¡œë”© (INT8 ì–‘ìí™”)...")
        
        # GPU ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ INT8 ë¡œë“œ
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            load_in_8bit=True,  # INT8 ì–‘ìí™”
            trust_remote_code=True
        )
        
        print("âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        
        # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                mem_used = torch.cuda.memory_allocated(i) / (1024**3)
                mem_total = torch.cuda.get_device_properties(i).total_memory / (1024**3)
                print(f"ğŸ® GPU {i}: {mem_used:.1f}/{mem_total:.1f} GB ì‚¬ìš©")
        
        return True
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return False

if __name__ == "__main__":
    # 1. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
    if "--download" in sys.argv or not Path("models/solar-10.7b").exists():
        success = download_solar_model()
        if not success:
            sys.exit(1)
    
    # 2. ë¡œë”© í…ŒìŠ¤íŠ¸
    if "--test" in sys.argv:
        test_model_loading()